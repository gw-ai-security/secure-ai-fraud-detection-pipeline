{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7195e836",
   "metadata": {},
   "source": [
    "# 02 – Feature Engineering (Fraud Detection)\n",
    "\n",
    "**Project:** Secure AI Fraud Detection Pipeline  \n",
    "**Purpose:** Build a robust feature pipeline for fraud detection following Privacy-by-Design principles.\n",
    "\n",
    "**Outputs of this notebook**\n",
    "- Time, amount, frequency, and contextual features\n",
    "- Preprocessing pipeline (`models/feature_pipeline.pkl`)\n",
    "- Feature names (`models/feature_names.json`)\n",
    "- Preprocessed dataset (`data/processed/features.parquet`)\n",
    "- Configuration file (`models/feature_config.json`)\n",
    "\n",
    "> Notes:  \n",
    "> - The notebook first looks for `data/processed/fraud_cleaned.csv` (from notebook 01) or falls back to `data/raw/fraud_simulated.csv`.  \n",
    "> - If neither is available, a **synthetic demo dataset** is generated for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20189fa-2dd9-4284-aa87-879198f064d0",
   "metadata": {},
   "source": [
    "## Block 2 – Imports & Project Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdf5bb61-a841-451d-952a-6287d38dc983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\n",
      "CLEAN_PATH   = C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\data\\processed\\fraud_cleaned.csv\n",
      "RAW_PATH     = C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\data\\raw\\fraud_simulated.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os, json, warnings, joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Determine project root robustly\n",
    "cwd = Path.cwd()\n",
    "PROJECT_ROOT = cwd if (cwd / \"data\").exists() else (cwd.parent if cwd.name == \"notebooks\" else cwd)\n",
    "\n",
    "# Define key paths\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "DATA_RAW       = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "MODELS         = PROJECT_ROOT / \"models\"\n",
    "\n",
    "# Ensure directories exist\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "CLEAN_PATH = DATA_PROCESSED / \"fraud_cleaned.csv\"\n",
    "RAW_PATH   = DATA_RAW / \"fraud_simulated.csv\"\n",
    "\n",
    "print(f\"PROJECT_ROOT = {PROJECT_ROOT}\")\n",
    "print(f\"CLEAN_PATH   = {CLEAN_PATH}\")\n",
    "print(f\"RAW_PATH     = {RAW_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcd5278-114a-4365-836c-7c0eb68a5687",
   "metadata": {},
   "source": [
    "## Block 3 – Data loading (with fallback & synthetic demo dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27aba852-515b-4fd7-88bd-f6750f53f7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\data\\processed\\fraud_cleaned.csv\n",
      "            timestamp  user_id  amount country event_type  hour  day  weekday  \\\n",
      "0 2025-07-24 10:15:00  USER123   199.5      DE      login    10   24        3   \n",
      "\n",
      "   amount_scaled  \n",
      "0            0.0  \n",
      "timestamp        datetime64[ns]\n",
      "user_id                  object\n",
      "amount                  float64\n",
      "country                  object\n",
      "event_type               object\n",
      "hour                      int64\n",
      "day                       int64\n",
      "weekday                   int64\n",
      "amount_scaled           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def _generate_synthetic(n: int = 5000, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a small synthetic fraud-like dataset for reproducible runs.\n",
    "    Saves it to data/raw/fraud_simulated.csv as a convenience.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    start = datetime(2024, 1, 1)\n",
    "\n",
    "    ts = [start + timedelta(minutes=int(x)) for x in rng.integers(0, 60*24*30, size=n)]\n",
    "    amount = np.round(rng.gamma(shape=2.0, scale=50.0, size=n), 2)\n",
    "    user_id = rng.integers(1000, 2000, size=n)\n",
    "    country = rng.choice(\n",
    "        [\"DE\",\"AT\",\"CH\",\"FR\",\"IT\",\"ES\",\"NL\",\"PL\",\"US\",\"GB\"],\n",
    "        size=n,\n",
    "        p=[.22,.08,.05,.12,.08,.08,.08,.09,.10,.10]\n",
    "    )\n",
    "    channel = rng.choice([\"app\",\"web\",\"pos\"], size=n, p=[.4,.4,.2])\n",
    "    merchant_category = rng.choice([\"grocery\",\"electronics\",\"travel\",\"gaming\",\"fashion\",\"other\"], size=n)\n",
    "\n",
    "    # Simple fraud label for later evaluation (optional downstream)\n",
    "    fraud = (\n",
    "        rng.random(size=n) < (\n",
    "            0.02\n",
    "            + 0.03*np.isin(country, [\"US\",\"GB\"])\n",
    "            + 0.02*(channel == \"web\")\n",
    "            + 0.04*(merchant_category == \"gaming\")\n",
    "            + 0.03*(amount > 300)\n",
    "        )\n",
    "    ).astype(int)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"timestamp\": ts,\n",
    "        \"amount\": amount,\n",
    "        \"user_id\": user_id,\n",
    "        \"country\": country,\n",
    "        \"channel\": channel,\n",
    "        \"merchant_category\": merchant_category,\n",
    "        \"is_fraud\": fraud,\n",
    "    })\n",
    "\n",
    "    # Persist for reuse\n",
    "    RAW_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(RAW_PATH, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the cleaned dataset if present, otherwise raw.\n",
    "    If neither exists, generate a synthetic dataset.\n",
    "    \"\"\"\n",
    "    if CLEAN_PATH.exists():\n",
    "        path = CLEAN_PATH\n",
    "    elif RAW_PATH.exists():\n",
    "        path = RAW_PATH\n",
    "    else:\n",
    "        print(\"No cleaned/raw CSV found – generating a synthetic demo dataset…\")\n",
    "        return _generate_synthetic()\n",
    "\n",
    "    print(f\"Loading data from: {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "# Load and normalize timestamp dtype\n",
    "df = load_data()\n",
    "\n",
    "if \"timestamp\" in df.columns:\n",
    "    try:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\", utc=False)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: could not parse 'timestamp' column:\", e)\n",
    "\n",
    "print(df.head())\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e914186e-338c-40f3-9434-e1c752f72d68",
   "metadata": {},
   "source": [
    "## Block 4 – Schema Detection (numeric, categorical, ID, and label columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1bd9fbe-a778-4ad0-8cf7-c7089e9619ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Schema Detection ===\n",
      "categorical: ['country']\n",
      "numeric    : ['amount']\n",
      "id-like    : ['user_id']\n",
      "label      : None\n",
      "\n",
      "Numeric preview (describe):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       amount\n",
       "count     1.0\n",
       "mean    199.5\n",
       "std       NaN\n",
       "min     199.5\n",
       "25%     199.5\n",
       "50%     199.5\n",
       "75%     199.5\n",
       "max     199.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top categories for 'country':\n",
      "country\n",
      "DE    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Note: No label found – unsupervised setting will be supported.\n"
     ]
    }
   ],
   "source": [
    "# Candidate lists – extend with your project’s actual column names if needed\n",
    "possible_categorical = [\n",
    "    \"country\", \"channel\", \"merchant_category\", \"device\", \"currency\", \"ts_daypart\"\n",
    "]\n",
    "possible_numeric = [\n",
    "    \"amount\", \"balance\", \"tx_count_1d\", \"tx_count_7d\", \"avg_amount_7d\",\n",
    "    \"amount_log1p\", \"freq_user_day\"\n",
    "]\n",
    "possible_id_like = [\"user_id\", \"account_id\", \"customer_id\", \"merchant_id\"]\n",
    "label_cols = [\"is_fraud\", \"label\", \"y\"]\n",
    "\n",
    "# Detect actual columns present in df\n",
    "categorical_cols = [c for c in possible_categorical if c in df.columns]\n",
    "numeric_cols     = [c for c in possible_numeric     if c in df.columns]\n",
    "id_cols          = [c for c in possible_id_like     if c in df.columns]\n",
    "label_col        = next((c for c in label_cols if c in df.columns), None)\n",
    "\n",
    "print(\"=== Schema Detection ===\")\n",
    "print(\"categorical:\", categorical_cols)\n",
    "print(\"numeric    :\", numeric_cols)\n",
    "print(\"id-like    :\", id_cols)\n",
    "print(\"label      :\", label_col)\n",
    "\n",
    "# Quick sanity stats (optional)\n",
    "if numeric_cols:\n",
    "    print(\"\\nNumeric preview (describe):\")\n",
    "    display(df[numeric_cols].describe())\n",
    "\n",
    "for c in categorical_cols:\n",
    "    print(f\"\\nTop categories for '{c}':\")\n",
    "    print(df[c].value_counts().head(10))\n",
    "\n",
    "# Hints if something is missing\n",
    "if not categorical_cols:\n",
    "    print(\"\\nNote: No categorical columns detected. Consider adding yours to 'possible_categorical'.\")\n",
    "if not numeric_cols:\n",
    "    print(\"\\nNote: No numeric columns detected. Consider adding yours to 'possible_numeric'.\")\n",
    "if not id_cols:\n",
    "    print(\"\\nNote: No ID-like column detected. Add e.g. 'customer_id' to 'possible_id_like'.\")\n",
    "if label_col is None:\n",
    "    print(\"\\nNote: No label found – unsupervised setting will be supported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2291d-53a0-4624-b29c-6bf732e26594",
   "metadata": {},
   "source": [
    "## Block 5 – Encoding categorical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e4c9003-542c-4edb-8695-5c3a1efead6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (1, 9)\n",
      "Encoded shape : (1, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>event_type</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>amount_scaled</th>\n",
       "      <th>country_DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-24 10:15:00</td>\n",
       "      <td>USER123</td>\n",
       "      <td>199.5</td>\n",
       "      <td>login</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  user_id  amount event_type  hour  day  weekday  \\\n",
       "0 2025-07-24 10:15:00  USER123   199.5      login    10   24        3   \n",
       "\n",
       "   amount_scaled  country_DE  \n",
       "0            0.0         1.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Block 5 – Encoding categorical features (version-safe + robust)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import inspect\n",
    "import numpy as np\n",
    "\n",
    "# Pick only columns that actually exist\n",
    "candidate_cats = [\"country\", \"transaction_type\", \"device_type\", \"ts_daypart\"]\n",
    "categorical_cols = [c for c in candidate_cats if c in df.columns]\n",
    "\n",
    "if not categorical_cols:\n",
    "    print(\"No categorical columns found among:\", candidate_cats)\n",
    "    features_encoded = df.copy()\n",
    "else:\n",
    "    # Handle sklearn versions: sparse_output (>=1.2) vs sparse (<1.2)\n",
    "    if \"sparse_output\" in inspect.signature(OneHotEncoder).parameters:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    else:\n",
    "        encoder = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "\n",
    "    # Fit/transform\n",
    "    encoded = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "    # Build encoded DataFrame\n",
    "    encoded_df = pd.DataFrame(\n",
    "        encoded,\n",
    "        columns=encoder.get_feature_names_out(categorical_cols),\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    # Join to original (drop raw categorical cols)\n",
    "    features_encoded = df.drop(columns=categorical_cols).join(encoded_df)\n",
    "\n",
    "print(\"Original shape:\", df.shape)\n",
    "print(\"Encoded shape :\", features_encoded.shape)\n",
    "features_encoded.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9987343-adc1-40fb-b5f2-58a61ceeab48",
   "metadata": {},
   "source": [
    "## Block 6 – Scaling numeric features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3416ccf5-5865-4283-ae62-61d139546571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling shape: (1, 8)\n",
      "After scaling shape : (1, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>event_type</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>country_DE</th>\n",
       "      <th>amount_scaled</th>\n",
       "      <th>hour_scaled</th>\n",
       "      <th>day_scaled</th>\n",
       "      <th>weekday_scaled</th>\n",
       "      <th>country_DE_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-24 10:15:00</td>\n",
       "      <td>USER123</td>\n",
       "      <td>199.5</td>\n",
       "      <td>login</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  user_id  amount event_type  hour  day  weekday  \\\n",
       "0 2025-07-24 10:15:00  USER123   199.5      login    10   24        3   \n",
       "\n",
       "   country_DE  amount_scaled  hour_scaled  day_scaled  weekday_scaled  \\\n",
       "0         1.0            0.0          0.0         0.0             0.0   \n",
       "\n",
       "   country_DE_scaled  \n",
       "0                0.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "# Identify label column (to exclude from scaling)\n",
    "possible_labels = [\"is_fraud\", \"label\", \"y\"]\n",
    "label_col = next((c for c in possible_labels if c in features_encoded.columns), None)\n",
    "\n",
    "# Exclude non-scalable columns\n",
    "exclude_cols = set([label_col] if label_col else []).union(\n",
    "    {\"timestamp\", \"date\", \"user_id\", \"account_id\", \"customer_id\", \"merchant_id\"}\n",
    ")\n",
    "\n",
    "# Find numeric columns eligible for scaling\n",
    "numeric_candidates = features_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_to_scale = [\n",
    "    c for c in numeric_candidates\n",
    "    if c not in exclude_cols and not c.endswith(\"_scaled\")\n",
    "]\n",
    "\n",
    "if not numeric_to_scale:\n",
    "    print(\"No numeric columns to scale.\")\n",
    "    features_scaled = features_encoded.copy()\n",
    "else:\n",
    "    scaler = RobustScaler()\n",
    "    scaled_array = scaler.fit_transform(features_encoded[numeric_to_scale])\n",
    "\n",
    "    # Create new DataFrame with \"_scaled\" suffix\n",
    "    scaled_df = pd.DataFrame(\n",
    "        scaled_array,\n",
    "        columns=[f\"{c}_scaled\" for c in numeric_to_scale],\n",
    "        index=features_encoded.index\n",
    "    )\n",
    "\n",
    "    # Drop any existing \"_scaled\" columns before joining (avoid duplicates)\n",
    "    overlapping = set(features_encoded.columns) & set(scaled_df.columns)\n",
    "    if overlapping:\n",
    "        features_encoded = features_encoded.drop(columns=list(overlapping))\n",
    "\n",
    "    features_scaled = features_encoded.join(scaled_df)\n",
    "\n",
    "    # Persist scaler for later use\n",
    "    try:\n",
    "        joblib.dump(scaler, MODELS / \"feature_scaler.pkl\")\n",
    "    except Exception as e:\n",
    "        print(\"Warning: could not save scaler:\", e)\n",
    "\n",
    "print(\"Before scaling shape:\", features_encoded.shape)\n",
    "print(\"After scaling shape :\", features_scaled.shape)\n",
    "features_scaled.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb4796-5b8f-4b0a-a024-d18f72a4931f",
   "metadata": {},
   "source": [
    "## Block 7 – Build preprocessing pipeline & export features/metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c9e8d9f-a10d-4b9f-b6cc-531da6eeefb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved feature matrix to: C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\data\\processed\\features.parquet\n",
      "Saved pipeline to: C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\models\\feature_pipeline.pkl\n",
      "Saved feature names to: C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\models\\feature_names.json\n",
      "Saved feature config to: C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\models\\feature_config.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>country_DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amount  hour  day  weekday  country_DE\n",
       "0     0.0   0.0  0.0      0.0         1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (1, 5)\n"
     ]
    }
   ],
   "source": [
    "# Block 7 – Build preprocessing pipeline & export features/metadata\n",
    "# - Creates a unified ColumnTransformer (OneHotEncoder + RobustScaler)\n",
    "# - Fits on engineered dataframe `df`\n",
    "# - Exports feature matrix (Parquet -> CSV fallback)\n",
    "# - Persists pipeline + feature names + config for training/inference\n",
    "\n",
    "import json, inspect\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Re-detect label and column sets to be safe\n",
    "possible_labels = [\"is_fraud\", \"label\", \"y\"]\n",
    "label_col = next((c for c in possible_labels if c in df.columns), None)\n",
    "\n",
    "# Categorical columns (reuse if defined earlier, else detect)\n",
    "if \"categorical_cols\" in globals() and categorical_cols:\n",
    "    cats = [c for c in categorical_cols if c in df.columns]\n",
    "else:\n",
    "    candidate_cats = [\"country\", \"channel\", \"merchant_category\", \"device\", \"currency\", \"ts_daypart\"]\n",
    "    cats = [c for c in candidate_cats if c in df.columns]\n",
    "\n",
    "# Numeric columns to scale (exclude IDs/timestamps/labels)\n",
    "exclude = {\"timestamp\", \"date\", \"user_id\", \"account_id\", \"customer_id\", \"merchant_id\"}\n",
    "if label_col: exclude.add(label_col)\n",
    "nums_all = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "nums = [c for c in nums_all if c not in exclude and not c.endswith(\"_scaled\")]\n",
    "\n",
    "# Version-safe OneHotEncoder\n",
    "if \"sparse_output\" in inspect.signature(OneHotEncoder).parameters:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "else:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Define ColumnTransformer\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", scaler, nums if nums else []),\n",
    "        (\"cat\", ohe,   cats if cats else []),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Select fit dataframe (only the columns used by the transformer)\n",
    "cols_for_fit = (nums if nums else []) + (cats if cats else [])\n",
    "if not cols_for_fit:\n",
    "    raise ValueError(\"No columns selected for preprocessing. Check your schema detection in earlier blocks.\")\n",
    "\n",
    "X_mat = preprocess.fit_transform(df[cols_for_fit])\n",
    "\n",
    "# Build feature names\n",
    "num_feature_names = nums\n",
    "cat_feature_names = list(preprocess.named_transformers_[\"cat\"].get_feature_names_out(cats)) if cats else []\n",
    "feature_names = num_feature_names + cat_feature_names\n",
    "\n",
    "# Assemble feature DataFrame\n",
    "features_df = pd.DataFrame(X_mat, columns=feature_names, index=df.index)\n",
    "if label_col:\n",
    "    features_df[label_col] = df[label_col].values\n",
    "\n",
    "# Export paths\n",
    "features_parquet = DATA_PROCESSED / \"features.parquet\"\n",
    "features_csv     = DATA_PROCESSED / \"features.csv\"\n",
    "pipeline_path    = MODELS / \"feature_pipeline.pkl\"\n",
    "featnames_path   = MODELS / \"feature_names.json\"\n",
    "featcfg_path     = MODELS / \"feature_config.json\"\n",
    "\n",
    "# Save feature matrix (Parquet preferred, CSV fallback)\n",
    "try:\n",
    "    import pyarrow  # noqa: F401\n",
    "    features_df.to_parquet(features_parquet, index=False)\n",
    "    saved_features_path = features_parquet\n",
    "except Exception as e:\n",
    "    print(\"Parquet export failed, falling back to CSV:\", e)\n",
    "    features_df.to_csv(features_csv, index=False)\n",
    "    saved_features_path = features_csv\n",
    "\n",
    "# Persist pipeline & metadata\n",
    "joblib.dump(preprocess, pipeline_path)\n",
    "\n",
    "with open(featnames_path, \"w\") as f:\n",
    "    json.dump(feature_names, f, indent=2)\n",
    "\n",
    "feature_config = {\n",
    "    \"categorical_cols\": cats,\n",
    "    \"numeric_cols\": nums,\n",
    "    \"label_col\": label_col,\n",
    "    \"used_columns_for_fit\": cols_for_fit\n",
    "}\n",
    "with open(featcfg_path, \"w\") as f:\n",
    "    json.dump(feature_config, f, indent=2)\n",
    "\n",
    "print(\"Saved feature matrix to:\", saved_features_path)\n",
    "print(\"Saved pipeline to:\", pipeline_path)\n",
    "print(\"Saved feature names to:\", featnames_path)\n",
    "print(\"Saved feature config to:\", featcfg_path)\n",
    "\n",
    "# Quick preview\n",
    "display(features_df.head())\n",
    "print(\"Feature matrix shape:\", features_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf976f76-7f97-4112-a869-7cd1edec3fc3",
   "metadata": {},
   "source": [
    "## Block 8 – Optional train/test split export (supervised or unsupervised)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17c1268f-42b0-41a2-aab9-a8fa82c580a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No label detected – exporting unsupervised split for development convenience.\n",
      "Saved:\n",
      " - C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\data\\processed\\X_train.parquet\n",
      " - C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\data\\processed\\X_test.parquet\n",
      "Shapes: (0, 5) (1, 5)\n"
     ]
    }
   ],
   "source": [
    "# Block 8 – Optional train/test split export\n",
    "# - If a label column exists: stratified train/test split and export\n",
    "# - If no label: export a shuffled holdout split for unsupervised workflows\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "\n",
    "# Ensure we have the feature frame from Block 7\n",
    "assert \"features_df\" in globals(), \"features_df not found. Run Block 7 first.\"\n",
    "\n",
    "# Paths\n",
    "X_train_pq = DATA_PROCESSED / \"X_train.parquet\"\n",
    "X_test_pq  = DATA_PROCESSED / \"X_test.parquet\"\n",
    "y_train_csv = DATA_PROCESSED / \"y_train.csv\"\n",
    "y_test_csv  = DATA_PROCESSED / \"y_test.csv\"\n",
    "\n",
    "# Identify label if present\n",
    "possible_labels = [\"is_fraud\", \"label\", \"y\"]\n",
    "label_col = next((c for c in possible_labels if c in features_df.columns), None)\n",
    "\n",
    "def _save_matrix(df: pd.DataFrame, path_parquet: Path) -> Path:\n",
    "    \"\"\"Save DataFrame to Parquet (preferred) with CSV fallback.\"\"\"\n",
    "    try:\n",
    "        import pyarrow  # noqa: F401\n",
    "        df.to_parquet(path_parquet, index=False)\n",
    "        return path_parquet\n",
    "    except Exception as e:\n",
    "        csv_path = path_parquet.with_suffix(\".csv\")\n",
    "        print(f\"Parquet export failed for {path_parquet.name}; falling back to CSV:\", e)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        return csv_path\n",
    "\n",
    "if label_col:\n",
    "    # Supervised case\n",
    "    print(f\"Label detected: {label_col} – creating stratified train/test split.\")\n",
    "    y = features_df[label_col].astype(int).values\n",
    "    X = features_df.drop(columns=[label_col])\n",
    "\n",
    "    # Guard: ensure both classes exist; if not, use standard split\n",
    "    if len(np.unique(y)) > 1:\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "        idx_train, idx_test = next(splitter.split(X, y))\n",
    "    else:\n",
    "        print(\"Only one class present; using random split without stratification.\")\n",
    "        idx_train, idx_test = train_test_split(\n",
    "            np.arange(len(X)), test_size=0.2, random_state=42, shuffle=True\n",
    "        )\n",
    "\n",
    "    X_train, X_test = X.iloc[idx_train], X.iloc[idx_test]\n",
    "    y_train, y_test = y[idx_train], y[idx_test]\n",
    "\n",
    "    # Persist\n",
    "    saved_X_train = _save_matrix(X_train, X_train_pq)\n",
    "    saved_X_test  = _save_matrix(X_test,  X_test_pq)\n",
    "    pd.Series(y_train, name=label_col).to_csv(y_train_csv, index=False)\n",
    "    pd.Series(y_test,  name=label_col).to_csv(y_test_csv,  index=False)\n",
    "\n",
    "    print(\"Saved:\")\n",
    "    print(\" -\", saved_X_train)\n",
    "    print(\" -\", saved_X_test)\n",
    "    print(\" -\", y_train_csv)\n",
    "    print(\" -\", y_test_csv)\n",
    "    print(\"Shapes:\", X_train.shape, X_test.shape, \" | Class balance (train/test):\",\n",
    "          np.mean(y_train).round(4), \"/\", np.mean(y_test).round(4))\n",
    "else:\n",
    "    # Unsupervised case\n",
    "    print(\"No label detected – exporting unsupervised split for development convenience.\")\n",
    "    # Shuffle and split 80/20 on rows\n",
    "    X = features_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "    n_test = max(1, int(0.2 * len(X)))\n",
    "    X_train, X_test = X.iloc[:-n_test], X.iloc[-n_test:]\n",
    "\n",
    "    saved_X_train = _save_matrix(X_train, X_train_pq)\n",
    "    saved_X_test  = _save_matrix(X_test,  X_test_pq)\n",
    "\n",
    "    print(\"Saved:\")\n",
    "    print(\" -\", saved_X_train)\n",
    "    print(\" -\", saved_X_test)\n",
    "    print(\"Shapes:\", X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1293dbb0-4056-49f3-b0e7-4c864d6ea13c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
