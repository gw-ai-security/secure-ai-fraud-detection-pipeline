{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7195e836",
   "metadata": {},
   "source": [
    "# 02 – Feature Engineering (Fraud Detection)\n",
    "\n",
    "**Project:** Secure AI Fraud Detection Pipeline  \n",
    "**Purpose:** Build a robust feature pipeline for fraud detection following Privacy-by-Design principles.\n",
    "\n",
    "**Outputs of this notebook**\n",
    "- Time, amount, frequency, and contextual features\n",
    "- Preprocessing pipeline (`models/feature_pipeline.pkl`)\n",
    "- Feature names (`models/feature_names.json`)\n",
    "- Preprocessed dataset (`data/processed/features.parquet`)\n",
    "- Configuration file (`models/feature_config.json`)\n",
    "\n",
    "> Notes:  \n",
    "> - The notebook first looks for `data/processed/fraud_cleaned.csv` (from notebook 01) or falls back to `data/raw/fraud_simulated.csv`.  \n",
    "> - If neither is available, a **synthetic demo dataset** is generated for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20189fa-2dd9-4284-aa87-879198f064d0",
   "metadata": {},
   "source": [
    "## Block 2 – Imports & Project Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdf5bb61-a841-451d-952a-6287d38dc983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\n",
      "CLEAN_PATH   = C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\data\\processed\\fraud_cleaned.csv\n",
      "RAW_PATH     = C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\data\\raw\\fraud_simulated.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os, json, warnings, joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Determine project root robustly\n",
    "cwd = Path.cwd()\n",
    "PROJECT_ROOT = cwd if (cwd / \"data\").exists() else (cwd.parent if cwd.name == \"notebooks\" else cwd)\n",
    "\n",
    "# Define key paths\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "DATA_RAW       = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "MODELS         = PROJECT_ROOT / \"models\"\n",
    "\n",
    "# Ensure directories exist\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "CLEAN_PATH = DATA_PROCESSED / \"fraud_cleaned.csv\"\n",
    "RAW_PATH   = DATA_RAW / \"fraud_simulated.csv\"\n",
    "\n",
    "print(f\"PROJECT_ROOT = {PROJECT_ROOT}\")\n",
    "print(f\"CLEAN_PATH   = {CLEAN_PATH}\")\n",
    "print(f\"RAW_PATH     = {RAW_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcd5278-114a-4365-836c-7c0eb68a5687",
   "metadata": {},
   "source": [
    "## Block 3 – Data loading (with fallback & synthetic demo dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27aba852-515b-4fd7-88bd-f6750f53f7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\data\\processed\\fraud_cleaned.csv\n",
      "Dataset too small (1 rows). Generating synthetic dataset with n=2000...\n",
      "            timestamp  amount  user_id country channel merchant_category  \\\n",
      "0 2024-01-03 16:15:00   44.02     1422      FR     app            travel   \n",
      "1 2024-01-24 05:14:00   85.59     1612      AT     pos       electronics   \n",
      "2 2024-01-20 15:17:00   76.67     1750      GB     web           fashion   \n",
      "3 2024-01-14 03:59:00  129.83     1753      FR     app       electronics   \n",
      "4 2024-01-13 23:46:00   32.56     1339      PL     pos            travel   \n",
      "\n",
      "   is_fraud  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "timestamp            datetime64[ns]\n",
      "amount                      float64\n",
      "user_id                       int64\n",
      "country                      object\n",
      "channel                      object\n",
      "merchant_category            object\n",
      "is_fraud                      int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def _generate_synthetic(n: int = 5000, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a small synthetic fraud-like dataset for reproducible runs.\n",
    "    Saves it to data/raw/fraud_simulated.csv as a convenience.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    start = datetime(2024, 1, 1)\n",
    "\n",
    "    ts = [start + timedelta(minutes=int(x)) for x in rng.integers(0, 60*24*30, size=n)]\n",
    "    amount = np.round(rng.gamma(shape=2.0, scale=50.0, size=n), 2)\n",
    "    user_id = rng.integers(1000, 2000, size=n)\n",
    "    country = rng.choice(\n",
    "        [\"DE\",\"AT\",\"CH\",\"FR\",\"IT\",\"ES\",\"NL\",\"PL\",\"US\",\"GB\"],\n",
    "        size=n,\n",
    "        p=[.22,.08,.05,.12,.08,.08,.08,.09,.10,.10]\n",
    "    )\n",
    "    channel = rng.choice([\"app\",\"web\",\"pos\"], size=n, p=[.4,.4,.2])\n",
    "    merchant_category = rng.choice([\"grocery\",\"electronics\",\"travel\",\"gaming\",\"fashion\",\"other\"], size=n)\n",
    "\n",
    "    # Simple fraud label for later evaluation (optional downstream)\n",
    "    fraud = (\n",
    "        rng.random(size=n) < (\n",
    "            0.02\n",
    "            + 0.03*np.isin(country, [\"US\",\"GB\"])\n",
    "            + 0.02*(channel == \"web\")\n",
    "            + 0.04*(merchant_category == \"gaming\")\n",
    "            + 0.03*(amount > 300)\n",
    "        )\n",
    "    ).astype(int)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"timestamp\": ts,\n",
    "        \"amount\": amount,\n",
    "        \"user_id\": user_id,\n",
    "        \"country\": country,\n",
    "        \"channel\": channel,\n",
    "        \"merchant_category\": merchant_category,\n",
    "        \"is_fraud\": fraud,\n",
    "    })\n",
    "\n",
    "    # Persist for reuse\n",
    "    RAW_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(RAW_PATH, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the cleaned dataset if present, otherwise raw.\n",
    "    If neither exists, generate a synthetic dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    if CLEAN_PATH.exists():\n",
    "        path = CLEAN_PATH\n",
    "    elif RAW_PATH.exists():\n",
    "        path = RAW_PATH\n",
    "    else:\n",
    "        print(\"No cleaned/raw CSV found – generating a synthetic demo dataset…\")\n",
    "        return _generate_synthetic()\n",
    "\n",
    "    print(f\"Loading data from: {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "# Load and normalize timestamp dtype\n",
    "df = load_data()\n",
    "\n",
    "MIN_ROWS = 2000\n",
    "\n",
    "if len(df) < MIN_ROWS:\n",
    "    print(f\"Dataset too small ({len(df)} rows). Generating synthetic dataset with n={MIN_ROWS}...\")\n",
    "    df = _generate_synthetic(n=MIN_ROWS, seed=42)\n",
    "\n",
    "\n",
    "if \"timestamp\" in df.columns:\n",
    "    try:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\", utc=False)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: could not parse 'timestamp' column:\", e)\n",
    "\n",
    "print(df.head())\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e914186e-338c-40f3-9434-e1c752f72d68",
   "metadata": {},
   "source": [
    "## Block 4 – Schema Detection (numeric, categorical, ID, and label columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1bd9fbe-a778-4ad0-8cf7-c7089e9619ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Schema Detection ===\n",
      "categorical: ['country']\n",
      "numeric    : ['amount']\n",
      "id-like    : ['user_id']\n",
      "label      : None\n",
      "\n",
      "Numeric preview (describe):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       amount\n",
       "count     1.0\n",
       "mean    199.5\n",
       "std       NaN\n",
       "min     199.5\n",
       "25%     199.5\n",
       "50%     199.5\n",
       "75%     199.5\n",
       "max     199.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top categories for 'country':\n",
      "country\n",
      "DE    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Note: No label found – unsupervised setting will be supported.\n"
     ]
    }
   ],
   "source": [
    "# Candidate lists – extend with your project’s actual column names if needed\n",
    "possible_categorical = [\n",
    "    \"country\", \"channel\", \"merchant_category\", \"device\", \"currency\", \"ts_daypart\"\n",
    "]\n",
    "possible_numeric = [\n",
    "    \"amount\", \"balance\", \"tx_count_1d\", \"tx_count_7d\", \"avg_amount_7d\",\n",
    "    \"amount_log1p\", \"freq_user_day\"\n",
    "]\n",
    "possible_id_like = [\"user_id\", \"account_id\", \"customer_id\", \"merchant_id\"]\n",
    "label_cols = [\"is_fraud\", \"label\", \"y\"]\n",
    "\n",
    "# Detect actual columns present in df\n",
    "categorical_cols = [c for c in possible_categorical if c in df.columns]\n",
    "numeric_cols     = [c for c in possible_numeric     if c in df.columns]\n",
    "id_cols          = [c for c in possible_id_like     if c in df.columns]\n",
    "label_col        = next((c for c in label_cols if c in df.columns), None)\n",
    "\n",
    "print(\"=== Schema Detection ===\")\n",
    "print(\"categorical:\", categorical_cols)\n",
    "print(\"numeric    :\", numeric_cols)\n",
    "print(\"id-like    :\", id_cols)\n",
    "print(\"label      :\", label_col)\n",
    "\n",
    "# Quick sanity stats (optional)\n",
    "if numeric_cols:\n",
    "    print(\"\\nNumeric preview (describe):\")\n",
    "    display(df[numeric_cols].describe())\n",
    "\n",
    "for c in categorical_cols:\n",
    "    print(f\"\\nTop categories for '{c}':\")\n",
    "    print(df[c].value_counts().head(10))\n",
    "\n",
    "# Hints if something is missing\n",
    "if not categorical_cols:\n",
    "    print(\"\\nNote: No categorical columns detected. Consider adding yours to 'possible_categorical'.\")\n",
    "if not numeric_cols:\n",
    "    print(\"\\nNote: No numeric columns detected. Consider adding yours to 'possible_numeric'.\")\n",
    "if not id_cols:\n",
    "    print(\"\\nNote: No ID-like column detected. Add e.g. 'customer_id' to 'possible_id_like'.\")\n",
    "if label_col is None:\n",
    "    print(\"\\nNote: No label found – unsupervised setting will be supported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2291d-53a0-4624-b29c-6bf732e26594",
   "metadata": {},
   "source": [
    "## Block 5 – Encoding categorical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e4c9003-542c-4edb-8695-5c3a1efead6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (1, 9)\n",
      "Encoded shape : (1, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>event_type</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>amount_scaled</th>\n",
       "      <th>country_DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-24 10:15:00</td>\n",
       "      <td>USER123</td>\n",
       "      <td>199.5</td>\n",
       "      <td>login</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  user_id  amount event_type  hour  day  weekday  \\\n",
       "0 2025-07-24 10:15:00  USER123   199.5      login    10   24        3   \n",
       "\n",
       "   amount_scaled  country_DE  \n",
       "0            0.0         1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Block 5 – Encoding categorical features (version-safe + robust)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import inspect\n",
    "import numpy as np\n",
    "\n",
    "# Pick only columns that actually exist\n",
    "candidate_cats = [\"country\", \"transaction_type\", \"device_type\", \"ts_daypart\"]\n",
    "categorical_cols = [c for c in candidate_cats if c in df.columns]\n",
    "\n",
    "if not categorical_cols:\n",
    "    print(\"No categorical columns found among:\", candidate_cats)\n",
    "    features_encoded = df.copy()\n",
    "else:\n",
    "    # Handle sklearn versions: sparse_output (>=1.2) vs sparse (<1.2)\n",
    "    if \"sparse_output\" in inspect.signature(OneHotEncoder).parameters:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    else:\n",
    "        encoder = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "\n",
    "    # Fit/transform\n",
    "    encoded = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "    # Build encoded DataFrame\n",
    "    encoded_df = pd.DataFrame(\n",
    "        encoded,\n",
    "        columns=encoder.get_feature_names_out(categorical_cols),\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    # Join to original (drop raw categorical cols)\n",
    "    features_encoded = df.drop(columns=categorical_cols).join(encoded_df)\n",
    "\n",
    "print(\"Original shape:\", df.shape)\n",
    "print(\"Encoded shape :\", features_encoded.shape)\n",
    "features_encoded.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9987343-adc1-40fb-b5f2-58a61ceeab48",
   "metadata": {},
   "source": [
    "## Block 6 – Scaling numeric features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3416ccf5-5865-4283-ae62-61d139546571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling shape: (1, 8)\n",
      "After scaling shape : (1, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>event_type</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>country_DE</th>\n",
       "      <th>amount_scaled</th>\n",
       "      <th>hour_scaled</th>\n",
       "      <th>day_scaled</th>\n",
       "      <th>weekday_scaled</th>\n",
       "      <th>country_DE_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-24 10:15:00</td>\n",
       "      <td>USER123</td>\n",
       "      <td>199.5</td>\n",
       "      <td>login</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  user_id  amount event_type  hour  day  weekday  \\\n",
       "0 2025-07-24 10:15:00  USER123   199.5      login    10   24        3   \n",
       "\n",
       "   country_DE  amount_scaled  hour_scaled  day_scaled  weekday_scaled  \\\n",
       "0         1.0            0.0          0.0         0.0             0.0   \n",
       "\n",
       "   country_DE_scaled  \n",
       "0                0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "# Identify label column (to exclude from scaling)\n",
    "possible_labels = [\"is_fraud\", \"label\", \"y\"]\n",
    "label_col = next((c for c in possible_labels if c in features_encoded.columns), None)\n",
    "\n",
    "# Exclude non-scalable columns\n",
    "exclude_cols = set([label_col] if label_col else []).union(\n",
    "    {\"timestamp\", \"date\", \"user_id\", \"account_id\", \"customer_id\", \"merchant_id\"}\n",
    ")\n",
    "\n",
    "# Find numeric columns eligible for scaling\n",
    "numeric_candidates = features_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_to_scale = [\n",
    "    c for c in numeric_candidates\n",
    "    if c not in exclude_cols and not c.endswith(\"_scaled\")\n",
    "]\n",
    "\n",
    "if not numeric_to_scale:\n",
    "    print(\"No numeric columns to scale.\")\n",
    "    features_scaled = features_encoded.copy()\n",
    "else:\n",
    "    scaler = RobustScaler()\n",
    "    scaled_array = scaler.fit_transform(features_encoded[numeric_to_scale])\n",
    "\n",
    "    # Create new DataFrame with \"_scaled\" suffix\n",
    "    scaled_df = pd.DataFrame(\n",
    "        scaled_array,\n",
    "        columns=[f\"{c}_scaled\" for c in numeric_to_scale],\n",
    "        index=features_encoded.index\n",
    "    )\n",
    "\n",
    "    # Drop any existing \"_scaled\" columns before joining (avoid duplicates)\n",
    "    overlapping = set(features_encoded.columns) & set(scaled_df.columns)\n",
    "    if overlapping:\n",
    "        features_encoded = features_encoded.drop(columns=list(overlapping))\n",
    "\n",
    "    features_scaled = features_encoded.join(scaled_df)\n",
    "\n",
    "    # Persist scaler for later use\n",
    "    try:\n",
    "        joblib.dump(scaler, MODELS / \"feature_scaler.pkl\")\n",
    "    except Exception as e:\n",
    "        print(\"Warning: could not save scaler:\", e)\n",
    "\n",
    "print(\"Before scaling shape:\", features_encoded.shape)\n",
    "print(\"After scaling shape :\", features_scaled.shape)\n",
    "features_scaled.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb4796-5b8f-4b0a-a024-d18f72a4931f",
   "metadata": {},
   "source": [
    "## Block 7 – Build preprocessing pipeline & export features/metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c9e8d9f-a10d-4b9f-b6cc-531da6eeefb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved feature matrix to: C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\data\\processed\\features.parquet\n",
      "Saved pipeline to: C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\models\\feature_pipeline.pkl\n",
      "Saved feature names to: C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\models\\feature_names.json\n",
      "Saved feature config to: C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\models\\feature_config.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>country_AT</th>\n",
       "      <th>country_CH</th>\n",
       "      <th>country_DE</th>\n",
       "      <th>country_ES</th>\n",
       "      <th>country_FR</th>\n",
       "      <th>country_GB</th>\n",
       "      <th>country_IT</th>\n",
       "      <th>country_NL</th>\n",
       "      <th>country_PL</th>\n",
       "      <th>country_US</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.503853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.011025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.116775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.513456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.639715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     amount  country_AT  country_CH  country_DE  country_ES  country_FR  \\\n",
       "0 -0.503853         0.0         0.0         0.0         0.0         1.0   \n",
       "1 -0.011025         1.0         0.0         0.0         0.0         0.0   \n",
       "2 -0.116775         0.0         0.0         0.0         0.0         0.0   \n",
       "3  0.513456         0.0         0.0         0.0         0.0         1.0   \n",
       "4 -0.639715         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   country_GB  country_IT  country_NL  country_PL  country_US  is_fraud  \n",
       "0         0.0         0.0         0.0         0.0         0.0         0  \n",
       "1         0.0         0.0         0.0         0.0         0.0         0  \n",
       "2         1.0         0.0         0.0         0.0         0.0         0  \n",
       "3         0.0         0.0         0.0         0.0         0.0         0  \n",
       "4         0.0         0.0         0.0         1.0         0.0         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (2000, 12)\n"
     ]
    }
   ],
   "source": [
    "# Block 7 – Build preprocessing pipeline & export features/metadata\n",
    "# - Creates a unified ColumnTransformer (OneHotEncoder + RobustScaler)\n",
    "# - Fits on engineered dataframe `df`\n",
    "# - Exports feature matrix (Parquet -> CSV fallback)\n",
    "# - Persists pipeline + feature names + config for training/inference\n",
    "\n",
    "import json, inspect\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Re-detect label and column sets to be safe\n",
    "possible_labels = [\"is_fraud\", \"label\", \"y\"]\n",
    "label_col = next((c for c in possible_labels if c in df.columns), None)\n",
    "\n",
    "# Categorical columns (reuse if defined earlier, else detect)\n",
    "if \"categorical_cols\" in globals() and categorical_cols:\n",
    "    cats = [c for c in categorical_cols if c in df.columns]\n",
    "else:\n",
    "    candidate_cats = [\"country\", \"channel\", \"merchant_category\", \"device\", \"currency\", \"ts_daypart\"]\n",
    "    cats = [c for c in candidate_cats if c in df.columns]\n",
    "\n",
    "# Numeric columns to scale (exclude IDs/timestamps/labels)\n",
    "exclude = {\"timestamp\", \"date\", \"user_id\", \"account_id\", \"customer_id\", \"merchant_id\"}\n",
    "if label_col: exclude.add(label_col)\n",
    "nums_all = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "nums = [c for c in nums_all if c not in exclude and not c.endswith(\"_scaled\")]\n",
    "\n",
    "# Version-safe OneHotEncoder\n",
    "if \"sparse_output\" in inspect.signature(OneHotEncoder).parameters:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "else:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Define ColumnTransformer\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", scaler, nums if nums else []),\n",
    "        (\"cat\", ohe,   cats if cats else []),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Select fit dataframe (only the columns used by the transformer)\n",
    "cols_for_fit = (nums if nums else []) + (cats if cats else [])\n",
    "if not cols_for_fit:\n",
    "    raise ValueError(\"No columns selected for preprocessing. Check your schema detection in earlier blocks.\")\n",
    "\n",
    "X_mat = preprocess.fit_transform(df[cols_for_fit])\n",
    "\n",
    "# Build feature names\n",
    "num_feature_names = nums\n",
    "cat_feature_names = list(preprocess.named_transformers_[\"cat\"].get_feature_names_out(cats)) if cats else []\n",
    "feature_names = num_feature_names + cat_feature_names\n",
    "\n",
    "# Assemble feature DataFrame\n",
    "features_df = pd.DataFrame(X_mat, columns=feature_names, index=df.index)\n",
    "if label_col:\n",
    "    features_df[label_col] = df[label_col].values\n",
    "\n",
    "# Export paths\n",
    "features_parquet = DATA_PROCESSED / \"features.parquet\"\n",
    "features_csv     = DATA_PROCESSED / \"features.csv\"\n",
    "pipeline_path    = MODELS / \"feature_pipeline.pkl\"\n",
    "featnames_path   = MODELS / \"feature_names.json\"\n",
    "featcfg_path     = MODELS / \"feature_config.json\"\n",
    "\n",
    "# Save feature matrix (Parquet preferred, CSV fallback)\n",
    "try:\n",
    "    import pyarrow  # noqa: F401\n",
    "    features_df.to_parquet(features_parquet, index=False)\n",
    "    saved_features_path = features_parquet\n",
    "except Exception as e:\n",
    "    print(\"Parquet export failed, falling back to CSV:\", e)\n",
    "    features_df.to_csv(features_csv, index=False)\n",
    "    saved_features_path = features_csv\n",
    "\n",
    "# Persist pipeline & metadata\n",
    "joblib.dump(preprocess, pipeline_path)\n",
    "\n",
    "with open(featnames_path, \"w\") as f:\n",
    "    json.dump(feature_names, f, indent=2)\n",
    "\n",
    "feature_config = {\n",
    "    \"categorical_cols\": cats,\n",
    "    \"numeric_cols\": nums,\n",
    "    \"label_col\": label_col,\n",
    "    \"used_columns_for_fit\": cols_for_fit\n",
    "}\n",
    "with open(featcfg_path, \"w\") as f:\n",
    "    json.dump(feature_config, f, indent=2)\n",
    "\n",
    "print(\"Saved feature matrix to:\", saved_features_path)\n",
    "print(\"Saved pipeline to:\", pipeline_path)\n",
    "print(\"Saved feature names to:\", featnames_path)\n",
    "print(\"Saved feature config to:\", featcfg_path)\n",
    "\n",
    "# Quick preview\n",
    "display(features_df.head())\n",
    "print(\"Feature matrix shape:\", features_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf976f76-7f97-4112-a869-7cd1edec3fc3",
   "metadata": {},
   "source": [
    "## Block 8 – Optional train/test split export (supervised or unsupervised)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17c1268f-42b0-41a2-aab9-a8fa82c580a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label detected: is_fraud – creating stratified train/test split.\n",
      "Saved:\n",
      " - C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\data\\processed\\X_train.parquet\n",
      " - C:\\Users\\admin\\Desktop\\AI Sec Project\\GitHub\\secure-ai-fraud-detection-pipeline\\notebooks\\data\\processed\\X_test.parquet\n",
      "Shapes: (1600, 11) (400, 11)\n"
     ]
    }
   ],
   "source": [
    "# --- Optional train/test split export ---\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "\n",
    "# Ensure we have the feature frame from Block 7\n",
    "assert \"features_df\" in globals(), \"features_df not found. Run Block 7 first.\"\n",
    "\n",
    "X_train_pq = DATA_PROCESSED / \"X_train.parquet\"\n",
    "X_test_pq  = DATA_PROCESSED / \"X_test.parquet\"\n",
    "\n",
    "possible_labels = [\"is_fraud\", \"label\", \"y\"]\n",
    "label_col = next((c for c in possible_labels if c in features_df.columns), None)\n",
    "\n",
    "def _save_matrix(df: pd.DataFrame, path: Path) -> Path:\n",
    "    try:\n",
    "        import pyarrow  # noqa\n",
    "        df.to_parquet(path, index=False)\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        csv_path = path.with_suffix(\".csv\")\n",
    "        print(f\"Parquet export failed for {path.name}; falling back to CSV:\", e)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        return csv_path\n",
    "\n",
    "\n",
    "if label_col:\n",
    "    # --- Supervised case ---\n",
    "    print(f\"Label detected: {label_col} – creating stratified train/test split.\")\n",
    "\n",
    "    y = features_df[label_col].astype(int).values\n",
    "    X = features_df.drop(columns=[label_col])\n",
    "\n",
    "    if len(np.unique(y)) > 1:\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "        idx_train, idx_test = next(splitter.split(X, y))\n",
    "    else:\n",
    "        print(\"Only one class present; using random split.\")\n",
    "        idx_train, idx_test = train_test_split(\n",
    "            np.arange(len(X)), test_size=0.2, random_state=42, shuffle=True\n",
    "        )\n",
    "\n",
    "    X_train, X_test = X.iloc[idx_train], X.iloc[idx_test]\n",
    "\n",
    "else:\n",
    "    # --- Unsupervised case (ROBUST) ---\n",
    "    print(\"No label detected – exporting unsupervised split for development convenience.\")\n",
    "\n",
    "    X = features_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "    n = len(X)\n",
    "\n",
    "    if n < 2:\n",
    "        raise ValueError(\n",
    "            f\"Need at least 2 rows for train/test split, got {n}. \"\n",
    "            \"Generate more data or load full dataset.\"\n",
    "        )\n",
    "\n",
    "    # ensure at least 1 train row and 1 test row\n",
    "    n_test = max(1, int(0.2 * n))\n",
    "    n_test = min(n_test, n - 1)\n",
    "\n",
    "    X_train, X_test = X.iloc[:-n_test], X.iloc[-n_test:]\n",
    "\n",
    "\n",
    "saved_X_train = _save_matrix(X_train, X_train_pq)\n",
    "saved_X_test  = _save_matrix(X_test,  X_test_pq)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", saved_X_train)\n",
    "print(\" -\", saved_X_test)\n",
    "print(\"Shapes:\", X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1293dbb0-4056-49f3-b0e7-4c864d6ea13c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (secure-ai)",
   "language": "python",
   "name": "secure-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
