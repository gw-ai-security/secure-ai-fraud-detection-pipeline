{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7195e836",
   "metadata": {},
   "source": [
    "# 02 – Feature Engineering (Fraud Detection)\n",
    "**Projekt:** Secure AI Fraud Detection Pipeline  \n",
    "**Zweck:** Robuste Feature-Pipeline für Fraud-Detection mit Privacy-by-Design-Grundsätzen.\n",
    "\n",
    "**Output dieses Notebooks**\n",
    "- Zeit-, Betrag-, Frequenz- und Kontext-Features\n",
    "- Scaler/Encoder-Pipeline (`models/feature_pipeline.pkl`)\n",
    "- Feature-Namen (`models/feature_names.json`)\n",
    "- Vorverarbeitete Daten (`data/processed/features.parquet`)\n",
    "- Konfig (`models/feature_config.json`)\n",
    "\n",
    "> Hinweise:  \n",
    "> - Das Notebook nutzt `data/processed/fraud_cleaned.csv` (falls vorhanden) oder `data/raw/fraud_simulated.csv`.  \n",
    "> - Wenn beides fehlt, wird ein **synthetischer Demo-Datensatz** erzeugt (für reproduzierbare Läufe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f5c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports und Pfade\n",
    "import os, json, warnings, joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "MODELS = PROJECT_ROOT / \"models\"\n",
    "\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLEAN_PATH = DATA_PROCESSED / \"fraud_cleaned.csv\"\n",
    "RAW_PATH = DATA_RAW / \"fraud_simulated.csv\"\n",
    "\n",
    "print(f\"PROJECT_ROOT = {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117a1e0",
   "metadata": {},
   "source": [
    "## 1. Daten laden (mit Fallback & synthetischem Demo-Datensatz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dfca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_synthetic(n=5000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    start = datetime(2024, 1, 1)\n",
    "    ts = [start + timedelta(minutes=int(x)) for x in rng.integers(0, 60*24*30, size=n)]\n",
    "    amount = np.round(rng.gamma(shape=2.0, scale=50.0, size=n), 2)\n",
    "    user_id = rng.integers(1000, 2000, size=n)\n",
    "    country = rng.choice([\"DE\",\"AT\",\"CH\",\"FR\",\"IT\",\"ES\",\"NL\",\"PL\",\"US\",\"GB\"], size=n, p=[.22,.08,.05,.12,.08,.08,.08,.09,.1,.1])\n",
    "    channel = rng.choice([\"app\",\"web\",\"pos\"], size=n, p=[.4,.4,.2])\n",
    "    merchant_category = rng.choice([\"grocery\",\"electronics\",\"travel\",\"gaming\",\"fashion\",\"other\"], size=n)\n",
    "    # Label für spätere Evaluation (optional)\n",
    "    fraud = (rng.random(size=n) < (\n",
    "        0.02 \n",
    "        + 0.03*(country.isin([\"US\",\"GB\"])) \n",
    "        + 0.02*(channel == \"web\") \n",
    "        + 0.04*(merchant_category == \"gaming\")\n",
    "        + 0.03*(amount > 300)\n",
    "    ).astype(float)).astype(int)\n",
    "    df = pd.DataFrame({\n",
    "        \"timestamp\": ts,\n",
    "        \"amount\": amount,\n",
    "        \"user_id\": user_id,\n",
    "        \"country\": country,\n",
    "        \"channel\": channel,\n",
    "        \"merchant_category\": merchant_category,\n",
    "        \"is_fraud\": fraud,\n",
    "    })\n",
    "    RAW_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(RAW_PATH, index=False)\n",
    "    return df\n",
    "\n",
    "def load_data():\n",
    "    if CLEAN_PATH.exists():\n",
    "        path = CLEAN_PATH\n",
    "    elif RAW_PATH.exists():\n",
    "        path = RAW_PATH\n",
    "    else:\n",
    "        print(\"Weder cleaned noch raw gefunden – generiere synthetischen Demo-Datensatz…\")\n",
    "        return _generate_synthetic()\n",
    "    print(f\"Lade Daten aus: {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "df = load_data()\n",
    "# Ensure timestamp dtype\n",
    "if \"timestamp\" in df.columns:\n",
    "    try:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\", utc=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(df.head())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e530f",
   "metadata": {},
   "source": [
    "## 2. Basisbereinigung & Schema-Erkennung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8bf90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spalten-Kandidaten (werden dynamisch geprüft)\n",
    "possible_categorical = [\"country\",\"channel\",\"merchant_category\",\"device\",\"currency\"]\n",
    "possible_numeric = [\"amount\",\"balance\",\"tx_count_1d\",\"tx_count_7d\",\"avg_amount_7d\"]\n",
    "possible_id_like = [\"user_id\",\"account_id\",\"customer_id\",\"merchant_id\"]\n",
    "label_cols = [\"is_fraud\",\"label\",\"y\"]\n",
    "\n",
    "# Effektiv vorhandene bestimmen\n",
    "categorical_cols = [c for c in possible_categorical if c in df.columns]\n",
    "numeric_cols = [c for c in possible_numeric if c in df.columns]\n",
    "id_cols = [c for c in possible_id_like if c in df.columns]\n",
    "label_col = next((c for c in label_cols if c in df.columns), None)\n",
    "\n",
    "print(\"Gefundene Spalten:\")\n",
    "print(\"categorical:\", categorical_cols)\n",
    "print(\"numeric:\", numeric_cols)\n",
    "print(\"id-like:\", id_cols)\n",
    "print(\"label:\", label_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef01d4",
   "metadata": {},
   "source": [
    "## 3. Zeit-Features & Betragstransformationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeitmerkmale\n",
    "if \"timestamp\" in df.columns:\n",
    "    df[\"ts_hour\"] = df[\"timestamp\"].dt.hour\n",
    "    df[\"ts_dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n",
    "    df[\"ts_is_weekend\"] = (df[\"ts_dayofweek\"] >= 5).astype(int)\n",
    "    # Tageszeit-Bin\n",
    "    df[\"ts_daypart\"] = pd.cut(df[\"ts_hour\"], bins=[-1,5,11,17,23], labels=[\"night\",\"morning\",\"afternoon\",\"evening\"])\n",
    "    if \"ts_daypart\" not in categorical_cols:\n",
    "        categorical_cols.append(\"ts_daypart\")\n",
    "\n",
    "# Betrag (robust gegen Ausreißer)\n",
    "if \"amount\" in df.columns:\n",
    "    df[\"amount_log1p\"] = np.log1p(df[\"amount\"])\n",
    "    numeric_cols = sorted(set(numeric_cols + [\"amount\",\"amount_log1p\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ce4ab",
   "metadata": {},
   "source": [
    "## 4. Häufigkeits- & Nutzer-Statistik-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ecd656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling/Frequenzmerkmale (vereinfachte, wenn timestamp vorhanden)\n",
    "if \"timestamp\" in df.columns and len(id_cols)>0:\n",
    "    id_col = id_cols[0]\n",
    "    df = df.sort_values([\"timestamp\"])\n",
    "    # Transaktionen pro Nutzer pro Tag\n",
    "    df[\"date\"] = df[\"timestamp\"].dt.date\n",
    "    tx_per_user_day = df.groupby([id_col,\"date\"])[\"timestamp\"].transform(\"count\")\n",
    "    df[\"freq_user_day\"] = tx_per_user_day.astype(int)\n",
    "    numeric_cols = sorted(set(numeric_cols + [\"freq_user_day\"]))\n",
    "\n",
    "# Ersatz für fehlende numerische Spalten\n",
    "for c in numeric_cols:\n",
    "    if df[c].dtype == \"O\":\n",
    "        # versuche zu konvertieren\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Fehlwerte simple Strategie\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median()) if numeric_cols else df\n",
    "for c in categorical_cols:\n",
    "    df[c] = df[c].fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631023ba",
   "metadata": {},
   "source": [
    "## 5. Preprocessing-Pipeline (OneHotEncoder + RobustScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(), numeric_cols if len(numeric_cols)>0 else []),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols if len(categorical_cols)>0 else []),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "feature_config = {\n",
    "    \"numeric_cols\": numeric_cols,\n",
    "    \"categorical_cols\": categorical_cols,\n",
    "    \"label_col\": label_col,\n",
    "    \"id_cols\": id_cols\n",
    "}\n",
    "\n",
    "# X / y bilden\n",
    "drop_cols = []\n",
    "if label_col is not None:\n",
    "    drop_cols.append(label_col)\n",
    "if \"timestamp\" in df.columns:\n",
    "    drop_cols.append(\"timestamp\")\n",
    "if \"date\" in df.columns:\n",
    "    drop_cols.append(\"date\")\n",
    "\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n",
    "y = df[label_col] if label_col in df.columns else None\n",
    "\n",
    "# Passende Feature-Liste für Transformer erzeugen\n",
    "X_for_fit = X[numeric_cols + categorical_cols] if (numeric_cols or categorical_cols) else X.select_dtypes(include=[np.number, \"object\"])\n",
    "\n",
    "Xt = preprocess.fit_transform(X_for_fit)\n",
    "\n",
    "# Feature-Namen rekonstruieren\n",
    "num_feats = numeric_cols\n",
    "cat_encoder = [t for t in preprocess.transformers_ if t[0]==\"cat\"]\n",
    "if cat_encoder and categorical_cols:\n",
    "    enc = cat_encoder[0][1]\n",
    "    cat_feats = list(enc.get_feature_names_out(categorical_cols))\n",
    "else:\n",
    "    cat_feats = []\n",
    "feature_names = list(num_feats) + cat_feats\n",
    "\n",
    "print(f\"Shape (features): {Xt.shape}, Num features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add811f",
   "metadata": {},
   "source": [
    "## 6. Artefakte & Daten exportieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38e2954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporte\n",
    "features_path = DATA_PROCESSED / \"features.parquet\"\n",
    "feature_pipeline_path = MODELS / \"feature_pipeline.pkl\"\n",
    "feature_names_path = MODELS / \"feature_names.json\"\n",
    "feature_config_path = MODELS / \"feature_config.json\"\n",
    "\n",
    "# Als Parquet speichern\n",
    "try:\n",
    "    import pyarrow  # noqa: F401\n",
    "    features_df = pd.DataFrame(Xt, columns=feature_names)\n",
    "    if y is not None:\n",
    "        features_df[label_col] = y.values\n",
    "    features_df.to_parquet(features_path, index=False)\n",
    "except Exception as e:\n",
    "    # Fallback zu CSV\n",
    "    features_path = DATA_PROCESSED / \"features.csv\"\n",
    "    features_df = pd.DataFrame(Xt, columns=feature_names)\n",
    "    if y is not None:\n",
    "        features_df[label_col] = y.values\n",
    "    features_df.to_csv(features_path, index=False)\n",
    "\n",
    "# Pipeline & Metadaten\n",
    "joblib.dump(preprocess, feature_pipeline_path)\n",
    "with open(feature_names_path, \"w\") as f:\n",
    "    json.dump(feature_names, f, indent=2)\n",
    "with open(feature_config_path, \"w\") as f:\n",
    "    json.dump(feature_config, f, indent=2)\n",
    "\n",
    "print(\"Gespeichert:\")\n",
    "print(\"-\", features_path)\n",
    "print(\"-\", feature_pipeline_path)\n",
    "print(\"-\", feature_names_path)\n",
    "print(\"-\", feature_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65514da2",
   "metadata": {},
   "source": [
    "## 7. Optional: Train/Test Split speichern (für 03_model_training.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce826d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "if y is not None:\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    idx_train, idx_test = next(sss.split(Xt, y.values))\n",
    "    X_train, X_test = Xt[idx_train], Xt[idx_test]\n",
    "    y_train, y_test = y.values[idx_train], y.values[idx_test]\n",
    "\n",
    "    # Parquet/CSV Export\n",
    "    def _export_split(arr, path):\n",
    "        try:\n",
    "            import pyarrow  # noqa\n",
    "            pd.DataFrame(arr).to_parquet(path, index=False)\n",
    "        except Exception:\n",
    "            path = path.with_suffix(\".csv\")\n",
    "            pd.DataFrame(arr).to_csv(path, index=False)\n",
    "\n",
    "    _export_split(X_train, DATA_PROCESSED / \"X_train.parquet\")\n",
    "    _export_split(X_test, DATA_PROCESSED / \"X_test.parquet\")\n",
    "    pd.Series(y_train).to_csv(DATA_PROCESSED / \"y_train.csv\", index=False)\n",
    "    pd.Series(y_test).to_csv(DATA_PROCESSED / \"y_test.csv\", index=False)\n",
    "\n",
    "    print(\"Train/Test Splits exportiert.\")\n",
    "else:\n",
    "    print(\"Kein Label gefunden – überspringe Train/Test Split (unüberwachtes Setting).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b49eee",
   "metadata": {},
   "source": [
    "## 8. Kurzer Qualitätscheck\n",
    "- Anzahl Features & Beispielzeilen\n",
    "- Verteilung zentraler numerischer Variablen\n",
    "- Cardinality der Kategorischen Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f2764",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature-Matrix Vorschau:')\n",
    "display(features_df.head())\n",
    "\n",
    "if len(feature_config.get(\"numeric_cols\", []))>0:\n",
    "    display(df[feature_config[\"numeric_cols\"]].describe())\n",
    "\n",
    "for c in feature_config.get(\"categorical_cols\", []):\n",
    "    print(f\"Top-Kategorien für {c}:\")\n",
    "    print(df[c].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5160b5e",
   "metadata": {},
   "source": [
    "---\n",
    "### Nächste Schritte (für `03_model_training.ipynb`)\n",
    "- IsolationForest oder andere Anomaly-Modelle auf `data/processed/X_train.parquet` trainieren\n",
    "- Modell & Threshold speichern (`models/isolation_forest.joblib`, `models/threshold.json`)\n",
    "- Explainability in `05_explainability_shap.ipynb` vorbereiten (auf wichtigste Merkmale)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
